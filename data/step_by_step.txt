Step by step to implement the proposed model:

1. Compress all the data: create_dataset.py in visual genome project
 - image_id and the image
 - utterance person: location x, y, w, h, id, and name
 - response person: location x, y, w, h, id, and name
  - question: a sentence
 - response: a sentence
  - object: location x, y, w, h, id, and name


2. Extract and pre-process data: data_loader1.py
 - image: keep the same (the images are already normalized to a same size)
 - utterance and response person: keep the location
 - topic: keep top 50 popular topics, the 50th topic is "UNK" including all the less popular topics
 - question concat with response -> conversation: Just count the conversation of the popular topics. If the conversation
 word count is larger than the threshold, add it the the conversation vocabulary. "UNK" is the last conversation word.
 We keep track the longest conversation length.
 - save image id, topic id, conversation list of ids of popular topics only to training data


3. Extract and save image feature: extract_fc7.py


4. Build a simple graph: simple_model.py
Input: conversation word ids, image extracted with VGG
Output: object (topic) id
Result: train 500, validation 200, Acc: 50%

.................I am here........................................


5. Extend the model: extend_model.py
Input: conversation word ids, image extracted with VGG
Output: bounding box position of the object (topic)
-- Visualize the bounding boxes and compare with the ground truth

6. Include people's location to see if it is better or not ? This is important!
