Step by step to implement the proposed model:

1. Compress all the data:
 - image_id and the image
 - utterance person: location x, y, w, h, id, and name
 - response person: location x, y, w, h, id, and name
  - question: a sentence
 - response: a sentence
  - object: location x, y, w, h, id, and name


2. Extract and pre-process data
 - image: keep the same (the images are already normalized to a same size)
 - utterance and response person: keep the location
 - topic: keep top 50 popular topics, the 50th topic is "UNK" including all the less popular topics
 - question concat with response -> conversation: Just count the conversation of the popular topics. If the conversation
 word count is larger than the threshold, add it the the conversation vocabulary. "UNK" is the last conversation word.
 We keep track the longest conversation length.
